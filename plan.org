* Objectives - Grep for github 
 - Scan github for example that create data structures from scratch (list, hash map, circular list, tree, array) 
   + keywords e.g list, insert, append ?
 - Use GitHub search to find possible examples using keywords, file endings, language restriction 
 - Pull repositories for examples 
 - Utilise regular expressions to find exact examples of data structure of interest being defined (linked list)
 

* Thoughts
 - 1000 results return for each unique query (may return diff results if diff sorting parameter used, indexed or best match )
 - What form do they take ?
   + pull entire project ?
   + run generic makefile to check for the present of thing to transform
	 - what is it that is being transformed or looked for presence of
	 - linked list implementation
 - Pulling each repo to run regex over is going to be slow 
 - limitations of github to 30 requests per minute for The Search API
 - Find file contents via various criteria. This method returns up to 100 results per page.
 - sort Default: best match
 - Simplest approach take the live-grep and feed the web server the repositories returned by the github code search (only uses keywords tho not true grep)
   + build 
 - having a local copy of github is insane so need to be smart use what github offers to reduce downloading repo locally for livegrep to search
 - GitHub Scaper to find interesting repo to clone and then regex (as GitHub Api will only return 1000 res)
 - Incrementaly go through number of stars definitly get diff list of 1000 repos each time :)
 - index files for repos are 5-6x larger than the repos :(
 - does livegrep have someway of paging results endlessly ? (max_matches exists but is that the best way)
   + limit number of repos searched then combine results of json (require fixed match) 
 - Clang filtering of repositories ? reduce the number of required repos to search across
 - Could build DB of every repo on github just having some information on it link to reposistory language information (seems possible)
 - Could build index of code on the fly from filtered list of repositories that are passed to code search tool (Repositories actual code could be stored locally if needed)
 - What are some local tools that are like grep for large set of files ? ([[https://github.com/google/codesearch][google code search tool]], [[https://beyondgrep.com/][ack - grep opt for programmers]], git grep, )
 - way to limit number of results from each repository
 - is there a better backend than livegreps codesearch
   + searchcode (server) does supply grep
   + hound is utilises  google code search which allows grep (not multiline) but has network protocol
   + google code search does not provide multiline and does not implement any kind of protocol (is a local set of tools)
 - assumption being made that normal local grep search across large collection of files is no efficient enough
   + should probably test this with the 1072 repos with 311M lines
 - the index.json file can utilise fully cloned repositories not just bare ones (ones that only have git object file like .pack and .idx) 

* Things I need to know ?
 - What is going to use the result of the tool ? 
   + Rejuvenation tools
   + People interested in examples of code
 - Is this just for transformation examples related to my project ?
 - How limited is the results return by github, can I even get all matches github made.
 - How to get more results return than just 1000
 - How slow can it be ?
 - How interactive does it need to be?
 - what is pulled by livegrep github index tool (whole repo), can you provide non github repos ? (must be able to)
 - can livesearch index not stored in git repo
 - livegrep json list of repos pulled stored in same location as repos
 - what is the security concern of donwloading random repos off the internet
 - do repos have unique ids ?
 - is there a limit on matching time for livegrep query (if so can I increase it or turn it off)
   + yes found option -timeout 
 - Is livegrep faster when query frontend api to get results (does effect match time ?)
 - Is backend quering using the grpc protocol quicker (what do you get back -> (json) )
 - livegrep: can it regex across lines
   + no
 - How much slower is git cloning --mirror than normal git clone (both set to depth 1) size differnce
 - Is it feasible to capture patterns that maybe linked lists in regex
 - Does the complexity of regex have a large impact on
 - how does using memory mapped files improve performance
 - 

   
* Links 
 - [[https://developer.github.com/v3/search/][GitHub Search api]]
   + The GitHub Search API provides up to 1,000 results for each search.
 - [[https://developer.github.com/v3/search/][GitHub Search api]]
   + to search code:  You must authenticate to search for code across all public repositories.
 - [[https://realpython.com/api-integration-in-python/][Restful APIs in python]]
 - [[https://github.com/PyGithub/PyGithub/issues/824][1000 request issue]]
 - [[https://www.alexdebrie.com/posts/faster-code-search-livegrep/][livegrep]] (pretty awesome)n
 - [[https://github.com/mamal72/telegram-github-search-bot][telegram github bot]]
 - [[https://github.com/mbcrawfo/GenericMakefile][Generic makefile]]
 - [[https://spin.atomicobject.com/2016/08/26/makefile-c-projects/][makefile c project]]
 - [[https://github.com/google/codesearch][google code search - set of commandline tools written in go]]
 - [[https://oracle.github.io/opengrok/][openGrok - source code search and cross reference engine]]
   + does it support regex => ([[https://github.com/oracle/opengrok/wiki/Comparison-with-Similar-Tools][only wildcards]])
 - [[https://searchcode.com/][Search code]] - searchcode server you can search across any repository of code that has been added by your administrator. 
   + does it support regex - (No regex)
	 - Search using wild-card operators E.G. search*,
	 - Search using boolean and search operators AND OR NOT ( ),
	 - Search using special characters E.G. i++;)
   + [[https://searchcode.com/?q=hello][searchcode website]] can search through repos from Github, Bitbucket, Google Code, CodePlex, GitLab, Sourceforge, Minix, Google , Gitorious 
	 so there must be away to get code info or repository info from each of these sites
 - Source graph (N/A)
 - [[http://www.lihaoyi.com/post/ReimaginingtheOnlineCodeExplorer.html][Reimagining the Online Code Explorer]] blog post (2017)
 - [[https://searchfox.org/][Search fox]], Create by mozila uses livegrep indexer, desc: Searchfox is a source code indexing tool for Mozilla Firefox
   + last update 2 years ago
   + gitrepo https://github.com/bgrins/searchfox has a nice diagram
 - [[https://github.com/hound-search/hound][Hound ]]: very simialr to livegrep build using Russ Cox: Regular Expression Matching with a Trigram Index.
   + "Create a config.json in a directory with your list of repositories."
   + supported version control systems Git, Mercurial, SVN, Bazaar can be specified per repo in json file
 - [[https://github.com/ggreer/the_silver_searcher][silver searcher]] - simliar to to ack and grep (supposedly faster)
   + ag --multiline
 - [[https://www.pcre.org/original/doc/html/pcregrep.html][pcre]] - multiline grep like tool
 - ripgrep - line oriented search tool
   + very fast but no multiline grep

	 
* Notes from trying stuff
 - using list as name of file leads to lot of cs student related learning reposistories
 - Backend and frontend deployed on server and but frontend can't communicate with backfront
   + direct calls to backend work using bloomRPC
   + does it work locally with bloomrpc ? (yes) once port is published
 - GITHUB API
   - If results returned by GitHub API are on multiple pages use ?page=3 to specify a page and ?per_page=100 to specify # max number on each page
   - For API requests using Basic Authentication or OAuth, you can make up to *5000* requests per hour.
   - For unauthenticated requests, the rate limit allows for up to *60* requests per hour. 
 - livegrep has specific file searching in a repo (could be used in conjunction with github api)
   + livegrep returns in code results and name file results
   + api for livegrep accessible https://livegrep.com/api/v1/search/ returns json, web query is https://livegrep.com/search/
   + custom indexing backend, uses Russ Cox's RE2 regex library
   + livegrep can timeout on large number of matches specified "why=TIMEOUT" but time spend is only 12s so could be extended sometime
	 + could be due to livegrep trying to be "live" to instantly return results
	 + there is a flag for setting timeout when starting codesearch
   + Walking HEAD like so  Walking repo_spec name=drx/kiwi, path=repos/drx/kiwi (including  submodules: false) is when index is being built for backend
   + commands like: codesearch_start -grpc localhost:9999  repos/livegrep.json with the repositories already download
	 - builds an in memory index for the backend for repo_pull_test5 which had 1 20.2G index file this is to large to be run
	 - this require an memory mapped index file
   + grep searches the named input FILEs for *lines* containing a match to the given PATTERN.
   + max number of livegrep returns on a simple query was 9100 matches when we had 800 repos in the repository
	 + larger number when we have less repos ? (No message size is limit )
	 + 
   + how to remove repos from search in query "hello repo:(Gateworks/linux-imx6|LeMaker/linux-sunxi)" uses regex actually
	 - kinda odd
   + ERROR on mass pull of 1200 repos error around repo 900
	 - GET https://api.github.com/repos/futureshocked/ArduinoSbS2017: 403 You have triggered an abuse detection mechanism. Please wait a few minutes before you try again. []
	 - staged git pull 
   + Size of message being return error is due to grpc see here: [[https://stackoverflow.com/questions/55362342/grpc-grpc-received-message-larger-than-max-8653851-vs-4194304-golang][grpc message error]]
	 - fix is here [[https://stackoverflow.com/questions/42629047/how-to-increase-message-size-in-grpc-using-python][extending message size grpc]]
	 - Need to change server side ?
   + ag
	 - ag "std::.*::iterator" -G ".cpp" repos/ > res_ag.txt
   + pcre
	 - have to feed pcre files to search (ag does not takes directory)
	 - find . -printf '"%h/%f"\n'  -iname '*.cpp'  | xargs pcregrep -M 'for\s*\([^;]*;[^;]*;[^)]\)' > res_pcgrep.txt
   + find repos -name "*.cpp" -or -name "*.h" -or -name "*.cc" -or -name "*.hpp" -or -name "*.cxx" -or -name "*.hxx" | wc -l
   + find repos -type f -name "*.cpp" -or -type f -name "*.h" -or -type f -name "*.cc" -or -type f -name "*.hpp" -or -type f -name "*.cxx" -or -type f -name "*.hxx" > file_names.txt
	 -  some people create directories with .hpp or .cpp at the end 0_o 
   + Initial search using local multiline grep tools like ag
   + get lines from middle of file: sed -n '1000,1100p;1101q' filename > newfile
   + ag suffers from very long return times when searching lots of repos with mult line grep (may only be on first run due to caching but still very bad)
   + codesearch can only selectiely search for one file type at a time :(
   + codesearch not all escape characters supported debug_error_string = "{"created":"@1560848879.747291311","description":"Error received from peer ipv4:127.0.0.1:9999","file":"src/core/lib/surface/call.cc","file_line":1046,"grpc_message":"line: invalid escape sequence: \1","grpc_status":3}"
	 - can't utilise capturing group references
   + can't get more than max_matches ~9000 maybe matches from codesearch to then multiline grep on
	 - would require some kind of paging system
	 

 
* Tasks
 1. Check the extent of github search functionality (write python code to get results and pull porjects)
	- how many keywords allowed to narrow search	

 2. How would pipeline work 1:
	- Users provide some restriction parameters that github can utilise to find repos/files
	- then User provides patterns of what they want to find (like grep) and they are applied to the repositories found earlier.
	  - how are regexs defined to identify linked list candidates?
	  - limited return of github maybe an issue
	  - time taken to download repository would be a problem ? 

 3. How would pipeline work 2:
	- regex provided	
	- then search made to GitHub api where regex is more specific
	- collect results from all searchs to the api and return them to the user

 4. Confirm Number of repositories GitHub Api can return (1000)
	- get more by using star count diff search 

 5. install livegrep on own machine using docker (stress test) (problematic failed to connect frontend to backend)
	- how to get livegrep to index a specific repo not a user
	  + docker run -v $(pwd):/data livegrep/indexer /livegrep/bin/livegrep-github-reindex -repo livegrep/livegrep -http -dir /data
	- test the backend codesearch running in docker with direct GRPD requests ([[https://github.com/uw-labs/bloomrpc][Bloomrpc]]) to ensure it works
	
 6. install livegrep on server using docker or build natively
	- install using blog post (done) or try livegrep repo docker install (trying local install to see if backend and talk to frontend and its just a docker error)
	- installed nativately and worked, built using bazel 0.26 slightly different from the one stated in projects github
	- Check different ways of build index of code or to run without building an index using native tool 
	- find way of passing repository from a list (or github api) to create an index for codesearch backend
	  + get top 1000 repo results for c language then  build index with the set of repos
	  + build a *JSON* file that specifies the required repos (THIS) (what are the formats for local repos, external repo on github (is this possible))
	  + use inbuilt github indexer and feed it a list a repos individually with flag -repo
	  + which is faster the github helper tool are starting codesearch directly with a index.json file ? (advantages/disadvantages)
	  + can livesearch index code not stored in git repo ?
	- Can you limit number of matches per user/repo/file so that the information is not
	- Can you perform multiline grep in livegrep
	  + underlying RE2 regex engine
		1. \n 	newline (≡ \012)
		2. $ at end of text (like \z not \Z) or line (m=true)
		3. ^ at beginning of text or line  
		4. \Z  at end of text, or before newline at end of text (NOT SUPPORTED)
	  + [[https://github.com/livegrep/livegrep/issues/27][Issue]]: "The livegrep index relies pretty heavily on the fact that searches only operate on single lines to enable usage of the index; 
        It'd be difficult to generalize it to multiline searches without some pretty deep changes, so it's not supported right now."
	- Livegrep does have a custom backend [[https://blog.nelhage.com/2015/02/regular-expression-search-with-suffix-arrays/][indexer]] does not use google codesearch this could be the reason for no multi line 

 7. How large is GitHub (tb or pb) 
	- [[https://github.com/search?q=is:public][29 Million public repos]], max size [[https://help.github.com/en/articles/what-is-my-disk-quota][100GB]] so size <2800tb, (need better estimate obviously)
	- 400,000 GitHub repositories, 1 billion files, 14 terabytes of code - [[https://medium.com/@hoffa/400-000-github-repositories-1-billion-files-14-terabytes-of-code-spaces-or-tabs-7cfe0b5dd7fd][Medium]] 
	- so maybe 1015tb if (29M/400,000)*14tb (still not good)
	- repos that are designated c or c++ number around 1M, c = 470k and c++ = 649K
	- 2.5*14tb= 35tb for c/c++ repos

 8. What API/tool exist for GitLab/sourceForge/bitbucket or other source code hosting sites (codeplex, google code)
	- GitLab: has search capabilities and a Restful API
	  + [[https://docs.gitlab.com/ee/user/search/advanced_global_search.html][Advanced Global Search]] (powered by Elasticsearch) is not yet available on GitLab.com
	  + over 1000 repos returned for simple search of ruby pages can keep being request by the looks of it (unlike github)
	- [[https://confluence.atlassian.com/bitbucket/use-the-bitbucket-cloud-rest-apis-222724129.html][BitBucket]]: Restful api & code search ?
	  + Wildcard searches (e.g. qu?ck buil*) are not supported.
	  + Regular expressions in queries are not supported, but you can search for special characters.
	- SourceForge: multiple APIs 	  
	  + Allura API - A read/write API for reading/writing to project info, tickets, wiki pages, etc
	  + Download Stats API - A read-only API for obtaining download statistics
	  + File Release API - set the default download for your project.

 9. How well does GitHub Identify repo language types (pretty well)

 10. Test regex for iterator and see results on large collection (done)
	 - Is the tool actually useful (some wa)

 11. stress test 1200 repos (done)
	 - Call to git clone to quickly is causing the problem
	 - limited by github [[https://developer.github.com/v3/#abuse-rate-limits][abuse rate limiting]] on the pulling of repositories in the livegrep-github-pull tool
	 - only pull ~800 repos at once

 12. Can implment multiline grep hack ?
	 - interface directly with the 

 13. Find way to get more than 9100 result if possible ?
	 - Error/limit
	   + limit for 800 repos seems to be as above (for simple requests like hello that return a lot)
	   + tried with "hello" for 100 repos but only got 4500 matches so can't try limit
	   + search term "he" in 100 repos limit was 8000 (lower than the search for "hello" in 800 repos limit 9100) (very odd)
		 - error message that gets returned { "error": "8 RESOURCE_EXHAUSTED: Received message larger than max (5296486 vs. 4194304)"}
		 - due to being 2 letter word ? ()
	   + Search term "init" in 100 repos: limit 11600 error: { "error": "8 RESOURCE_EXHAUSTED: Received message larger than max (4197748 vs. 4194304)"}
	 - Solutions
	   - pagnation of some sort ask for first 9000 (or what ever limit is then next amount not including already requested)
	   - can limit then change what repos are being searched to produce distinct result below limit build up response
		 + kind of a hack tho not real fix

 14. Generic make files test on results from a search
	 - Start by end of day 

	   
* Additional
  - check real-world implementations of linked lists how different people define them.
  - download top page of repositories instead of stars (what effect it makes)
  - difference in time between my tool and livegrep tool for pulling github repos

	
* TOD0
  1) 1200 repo test on server (fail) (pull repos in 2 batches 800 then 400 after some failing final result 1072 repos)
	 - really need to know limit of codesearch server by need of the week and if it needs replaced
	 - need to test again to see how large an index can be actually used (current 1000 > 3000 ?)
	 - how do complex regex patterns effect the searching through the index ?
	   + does it cause memory to be overwelmed (works for 9000 results from repo of )
  2) Add multiline grep functionality (1) (3h)
	 - Side get list of repos with results return from search
	 - Get context around first line hit in multigrep specfic number of lines of rest of grep to check
	 - RE2 in python to use mutil-line (other tools are useable RE2 is quick tho)
	 - Need way to directly access server from python quickly grpc python lib (done)
	 - return number of lines needed for multiline grep (wild card .* regex that can match to \n means every line needs to be taken into account?)
	 - Line offset in to file (repo file is remote)
  3) (done) build script for pulling repos, limited by the livegrep tool to 800~900 (i think) pull repos with depth 1 then have to build json file for codesearch (2)
	 - or add more code to wrapper to stop the thing from hitting limit pull repos in batches of 800
	 - Double check existing livegrep tool does not have a way around this 
	 - Think about what is required to alter the go source code to build one
  4) Fix Limit problem on results being 8000 - 11600 seems to vary a bit something to do with message size being larger than max (3)
	 - problem woth grpc and needs to be fixed server side pull new copy of code and edit it ?
	 - or edit to implement pagnation
  5) regex for finding iterators, for loops ?, linked list ? (done)	 
  =====
  1) create regexs (for loops, linked list, structs) (done)
  2) 
  3) Testing tools (ag/codeseach) with regexs (doing)
  4) Generic makefile to build projects that are return by regex search (4)

  5) built own ag for better control
	 - Timeout, get results in any format,
	 - Is single thread re2 regex quick enough (what are the limiting factors: python read file? printing/IO)
	 - Need to parallelise it, to check each file in parallel 8 processes ? 
	   + how to handle cutting up of files to search: have list of files, search independently through repos
	 - Features (1)
	   + Stop search after a set time (done)
	   + Stop search after a certain number of matches (done)
	   + Better way of locating files of interest than from file of names (done)
	   + Limit Number of matches in a file ()
	   + Pagenation of results
		 + search already launch with pattern with either max_matches set or timelimit set do you want to get next set of results ? 
		 + start search at file after last one processed  (needs some storage measure or order to files searched to be able to return to a specific location in a search)
		 + json file of searched for patterns and where to in the search they got
	   + Be able to limit search of repositories by some metrics: size, # of stars, language: c|c++, certain file endings (done)
		 * select repos to search based on tags ?
		 * have to build a database of info based on repos
	   + return line numbers ? (not doable with re2 python module :( )
  6) Build database of repositories
	 - query database to return a list (generator ?) of repositories of interest than are then searched instead of the entire set of repositories downloaded
	 - 
  7) Format of results
  8) tools for building regex expressions
  9) C preprocessor marco for regex (2)
  10) Some form of filtering of repositories before hand (info in a database stars)
  11) how to create some form of limitation of search or pick off from where you started (list of file names know where to start off)
  12) how long does it take to run over all the repositories
	  - time $(ag --cpp --multiline "std::.*::iterator" repos/ > ag_res.txt)
         real	30m6.441s
         user	0m43.096s
		 sys	1m43.102s
	  - time $(ag --cpp --multiline "struct\s+([\w_][\w0-9_]*)\s*{(\s*[\w_][\w0-9_]*\s+[\w_][\w0-9_]*\s*;\s*)*(\1((\s*\*\s*)*|\s*&\s*&?\s*|\s+)[\w_][\w0-9_]*\s*;\s*)(\s*[\w_][\w0-9_]*\s+[\w_][\w0-9_]*;\s*)*\s*};" repos/ > ag_res.txt)
		real	30m56.252s
		user	5m56.449s
		sys	1m17.087s
  13) what is next: try injecting transformations on source code into the build system?
  14) combine github script to create a single github repository retriver, cloner, and database builder for the tool
	 
	  
* Programme:
  - Get repo data from Github
  - build data base (ensure repositories are unique) (done)
	- parameters to build repository limit language
  - use repository data to clone all github repos locally (done)
	- need to work in parallel to pull repositories
	- query data base to get list
  - Interface to database to filter repos that get searched (done)
	- search for repos based on stars, language etc
	- get results back in from of a list 


* Repository filtering
** Information to store
   - Name of user
   - Name of repository
   - Number of stars
   - size of repository
   - Major language of repository e.i cpp, c, html, etc...
   - year
   - topics ? 

** Relational Database
   - Need to build database when pulling the repositorys locally (or build from a config file )
   - mysql vs sqlite3 
	 - sqlite3 [[https://www.sqlite.org/limits.html][limits of sqlite3 as a rdms]]
   - How to build database have to edit repo puller ? 
** Json nosql approach
   - 
   -
   - 


* Generic Make investigation 
** Software found
   - [[https://en.wikipedia.org/wiki/Automake][automake]]
   - autoconf tools (automake part of it)
   - cmake
   - gcc compiler dependency generators
** Notes  
   - some repos don't even have makefiles present
   - Can other build system automate make process easier
   - what about makefiles with multiple targets
   - externals dependicies dynamically linked libraries
   - majority language might not indicate project is to be made in that language
	 - found what seems to be an objective c project classed as c due to use of .h files i think
   - Ignore install make rules ?
   - have to identitfy linked libraries used in project and add them
   - Take generic makefile and project as arguments to a script that produces a more specific makefile for that project
   - Problem dealing with projects with multiple makefiles one for macos one for linux can a simple generic one actually work
   - what about platorm specific projects not ment to be built on unix systems
   - Hard coded src directory effective ? (would avoid problems with tests defining main, could blacklist test directory ?)
   - what about tools like autoconfig
   - try to only select many src directory may yield better results
   - How to deal with windows specific files 
** Things learned from trying stuff
   - how to even test if system has been built properly
	 - no make errors ? (scan makefile output)
   - mutilple definitions of main due to testing code also being built
   - multiple tools all with own make file in repository
   - Makefile which has a rule which calls a make file in another directory

** Things i need to know
   - How functional do the makefiles have to be: do they have to build te library/executable completely
** THINGS TODO
   - test generic makefiles on c and c++ repos to see if they work (2 hours) (db has 47 as test set) (done)
   - find variations of generic makefiles (done)
	 - [[https://github.com/Cheedoong/MakefileTemplate/blob/master/Makefile][cheedoong]] makefile
	 - Vranish makefile: [[https://spin.atomicobject.com/2016/08/26/makefile-c-projects/][A Super-Simple Makefile for Medium-Sized C/C++ Projects]] 
   - testing/build of repositories (done)
   - improve makefile successful compile rate
	 - aim to get simplest cases to compile
	 - may have to write own makefile
   - try on large set of 3000 repos
	 - record rate (improve further)

   - New way of moving forward
	 - Aim to just compile files to object ".o" by hooking in clang tool before compilation (make it modular ?)
	 - Report passes or fails in csv output what stage reached type of success
	   - 3 types of outcome Not successful, tool fell over, found pattern / actually transformed
	 - Output of tool 
